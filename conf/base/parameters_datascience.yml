model_params:
  # Selected model for training 
  # "decision_tree" | "ada_boost" | "xg_boost" | "mlp" | "cat_boost" | "knn" | "lightgbm"
  model_choice: "cat_boost"
  # Global random state seed for model
  random_state: 0
  alpha: 0.6
  # Whether to save the model
  save_model: False
  save_model_name: "cat_model"

  # DECISION TREE
  # Use bayesian optimisation to auto select best parameters: True | False
  decision_tree_auto_optimize: True
  decision_tree_bayes_search_settings:
    # Keyword arguments for bayesian optimisation for scikit-optimize
    # scoring: "f1"
    n_iter: 50
    cv: 5
  decision_tree_bayes_search_search_space:
    # Search space for decision tree bayesian optimisation
    max_depth: [1, 20]
    min_samples_split: [2, 20]
    min_samples_leaf: [1, 10]
    criterion: ['gini', 'entropy']
  decision_tree_settings:
    # Keyword arguments for scikit-learn DecisionTreeClassifier
    max_depth: 3

  # CAT BOOST
  cat_boost_auto_optimize: False
  cat_boost_grid_search_settings:
    n_iter: 50
    cv: 5
  cat_boost_grid_search_search_space:
    depth: [4, 5, 6, 7, 8, 9, 10]
    learning_rate: [0.01, 0.03, 0.05]
    l2_leaf_reg: [2, 3, 4, 5, 6, 7]
  cat_boost_settings:
    iterations: 1000
    task_type: "CPU"
    loss_function: "MultiClass"
    # class_weights: [1, 2.5]
    auto_class_weights: "Balanced"
    # use_best_model: True

  # ADA BOOST
  ada_boost_auto_optimize: False
  ada_boost_bayes_search_settings:
    # Keyword arguments for bayesian optimisation for scikit-optimize
    n_iter: 50
    cv: 5
  ada_boost_bayes_search_search_space:
    # Search space for adaboost bayesian optimisation
    n_estimators: [50, 60]
    learning_rate: [0.9, 1.0]
  adaboost_settings:
    n_estimators: 50
    learning_rate: 1

  # LIGHTGBM
  lightgbm_auto_optimize: False
  lightgbm_bayes_search_settings:
    # Keyword arguments for bayesian optimisation for scikit-optimize
    n_iter: 50
    n_jobs: 5
    cv: 5
  lightgbm_bayes_search_search_space:
    # Search space for adaboost bayesian optimisation
    num_leaves: [20, 40]
    max_depth: [-1, 10]
  lightgbm_settings:
    boosting_type: "gbdt"
    num_leaves: 31
    max_depth: -1

  # KNN
  knn_auto_optimize: True
  knn_bayes_search_settings:
    # Keyword arguments for bayesian optimisation for scikit-optimize
    n_iter: 50
    cv: 3
  knn_bayes_search_search_space:
    # Search space for decision tree bayesian optimisation
    n_neighbors: [1, 20]
    weights: ["distance", "uniform"]
  knn_settings:
    n_neighbors: 3
    weights: "distance"

  # XG BOOST
  xgboost_auto_optimize: False
  xgboost_bayes_search_settings:
    n_iter: 50
    cv: 5
  xgboost_bayes_search_search_space:
    max_depth: [3, 10]
    learning_rate: [0.01, 0.3]
    n_estimators: [100, 500]
    subsample: [0.5, 1.0]
    colsample_bytree: [0.5, 1.0]
    reg_lambda: [0.0, 5.0]
    reg_alpha: [0.0, 5.0]
  xgboost_settings:
    objective: "binary:logistic"
    eval_metric: "logloss"
    max_depth: 6
    learning_rate: 0.1
    n_estimators: 300
    subsample: 0.8
    colsample_bytree: 0.8
    scale_pos_weight: 4.0
