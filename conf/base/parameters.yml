cleaning_params:
  # AGE
  # Create the age_unk columns (True, False)
  create_age_unk: False
  # Do KNN imputation to impute missing ages (True, False)
  age_knn_impute: True
  age_knn_impute_settings:  
    # Keyword arguments for KNNImputer from sklearn.impute
    n_neighbors: 2
    weights: "distance"

  # OCCUPATION
  generalize: False

  # PREVIOUS CONTACT DAYS
  # Create previously_contacted column (True, False)
  create_previously_contacted: True

  # PERSONAL_LOAN 
  # Cleaning mode for personal_loan column ('fill', 'drop', 'impute')
  personal_loan_cleaning: "fill"

feature_selection_params:
  # Naive ranking of features made from EDA
  ranking: ['occupation', 'contact_method', 'credit_default', 'previously_contacted', 'campaign_calls', 'marital_status', 'education_level', 'age', 'age_unk', 'housing_loan', 'personal_loan']
  # Select top features
  top_n_features: 11

encoding_params:
  # Options: ('ohe', 'label', 'none')
  # none should only be used for Catboost
  encode: "none"

splitting_params:
  random_state: 0
  # Float value representing % of dataset to be in test split
  test_size: 0.2
  # Options: ('undersampling', 'stratified', 'oversampling')
  imbalance_handling: 'oversampling'
  sampling_strategy: 1.0

model_params:
  # Selected model for training 
  # "decision_tree" | "ada_boost" | "xg_boost" | "mlp" | "cat_boost" | "knn"
  model_choice: "cat_boost"
  # Global random state seed for model
  random_state: 0
  # Whether to save the model
  save_model: False
  save_model_name: "mlp_model"

  # DECISION TREE
  # Use bayesian optimisation to auto select best parameters: True | False
  decision_tree_auto_optimize: True
  decision_tree_bayes_search_settings:
    # Keyword arguments for bayesian optimisation for scikit-optimize
    # scoring: "f1"
    n_iter: 50
    cv: 5
  decision_tree_bayes_search_search_space:
    # Search space for decision tree bayesian optimisation
    max_depth: [1, 20]
    min_samples_split: [2, 20]
    min_samples_leaf: [1, 10]
    criterion: ['gini', 'entropy']
  decision_tree_settings:
    # Keyword arguments for scikit-learn DecisionTreeClassifier
    max_depth: 3

  # CAT BOOST
  cat_boost_auto_optimize: False
  cat_boost_grid_search_settings:
    n_iter: 50
    cv: 5
  cat_boost_grid_search_search_space:
    depth: [4, 5, 6, 7, 8, 8, 9, 10]
    learning_rate: [0.01, 0.025, 0.05, 0.075, 0.1]
    l2_leaf_reg: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
  cat_boost_settings:
    iterations: 1000
    depth: 10
    learning_rate: 0.01
    loss_function: 'Logloss'
    eval_metric: 'AUC'
    early_stopping_rounds: 50
    verbose: 100
    task_type: "CPU"

  # ADA BOOST
  adaboost_settings:
    n_estimators: 1000
    learning_rate: 0.01

  # KNN
  knn_auto_optimize: True
  knn_bayes_search_settings:
    # Keyword arguments for bayesian optimisation for scikit-optimize
    n_iter: 50
    cv: 3
  knn_bayes_search_search_space:
    # Search space for decision tree bayesian optimisation
    n_neighbors: [1, 20]
    weights: ["distance", "uniform"]
  knn_settings:
    n_neighbors: 3
    weights: "distance"

  # XG BOOST
  xgboost_settings:
    n_estimators: 200
    learning_rate: 0.1
    max_depth: 6
    objective: "binary:logistic"
    tree_method: "hist"
    subsample: 0.8
    colsample_bytree: 0.8

  mlp_settings:
    hidden_layers: [128, 128, 128]
    lr: 0.01
    epochs: 1000 
