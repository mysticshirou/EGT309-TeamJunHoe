cleaning_params:
  # AGE
  # Create the age_unk columns (True, False)
  create_age_unk: False
  # Do KNN imputation to impute missing ages (True, False)
  age_knn_impute: True
  age_knn_impute_settings:  
    # Keyword arguments for KNNImputer from sklearn.impute
    n_neighbors: 2
    weights: "distance"

  # OCCUPATION
  generalize: False

  # PREVIOUS CONTACT DAYS
  # Create previously_contacted column (True, False)
  create_previously_contacted: True

  # PERSONAL_LOAN 
  # Cleaning mode for personal_loan column ('fill', 'drop', 'impute')
  personal_loan_cleaning: "drop"

encoding_params:
  # Options: ('ohe', 'label')
  encode: "ohe"

splitting_params:
  # Float value representing % of dataset to be in test split
  test_size: 0.2
  # Options: ('undersampling', 'stratified', 'class_weights')
  imbalance_handling: 'undersampling'

model_params:
  # Selected model for training 
  # "decision_tree" | "ada_boost" | "xg_boost" | "mlp"
  model_choice: "mlp"
  # Global random state seed
  random_state: 0

  # DECISION TREE
  # Use bayesian optimisation to auto select best parameters: True | False
  decision_tree_auto_optimize: True
  decision_tree_bayes_search_settings:
    # Keyword arguments for bayesian optimisation for scikit-optimize
    # scoring: "f1"
    n_iter: 50
    cv: 5
  decision_tree_bayes_search_search_space:
    # Search space for decision tree bayesian optimisation
    max_depth: [1, 20]
    min_samples_split: [2, 20]
    min_samples_leaf: [1, 10]
    criterion: ['gini', 'entropy']
  decision_tree_settings:
    # Keyword arguments for scikit-learn DecisionTreeClassifier
    max_depth: 3

  # ADA BOOST
  adaboost_settings:
    n_estimators: 100

  # XG BOOST
  xgboost_settings:
    n_estimators: 200
    learning_rate: 0.1
    max_depth: 6
    objective: "binary:logistic"
    tree_method: "hist"
    subsample: 0.8
    colsample_bytree: 0.8

  mlp_setting:
    hidden_layers: [64, 64, 64]
    lr: 0.01
    epochs: 100 
