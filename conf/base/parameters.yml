cleaning_params:
  # AGE
  # Create the age_unk columns (True, False)
  create_age_unk: False
  # Do KNN imputation to impute missing ages (True, False)
  age_knn_impute: True
  age_knn_impute_settings:  
    # Keyword arguments for KNNImputer from sklearn.impute
    n_neighbors: 2
    weights: "distance"

  # OCCUPATION
  generalize: False

  # PREVIOUS CONTACT DAYS
  # Create previously_contacted column (True, False)
  create_previously_contacted: True

  # PERSONAL_LOAN 
  # Cleaning mode for personal_loan column ('fill', 'drop', 'impute')
  personal_loan_cleaning: "fill"

feature_selection_params:
  # Naive ranking of features made from EDA
  ranking: ['occupation', 'contact_method', 'credit_default', 'campaign_calls', 'marital_status', 'education_level', 'age', 'housing_loan', 'personal_loan']
  # Select top features (9 in total)
  top_n_features: 6

encoding_params:
  # Options: ('ohe', 'label', 'none')
  # none should only be used for Catboost
  encode: "ohe"

splitting_params:
  # Float value representing % of dataset to be in test split
  test_size: 0.2
  # Options: ('undersampling', 'stratified')
  imbalance_handling: 'undersampling'

model_params:
  # Selected model for training 
  # "decision_tree" | "ada_boost" | "xg_boost" | "mlp"
  model_choice: "decision_tree"
  # Global random state seed
  random_state: 0
  # Whether to save the model
  save_model: True
  save_model_name: "decision_tree_model"

  # DECISION TREE
  # Use bayesian optimisation to auto select best parameters: True | False
  decision_tree_auto_optimize: True
  decision_tree_bayes_search_settings:
    # Keyword arguments for bayesian optimisation for scikit-optimize
    # scoring: "f1"
    n_iter: 50
    cv: 5
  decision_tree_bayes_search_search_space:
    # Search space for decision tree bayesian optimisation
    max_depth: [1, 20]
    min_samples_split: [2, 20]
    min_samples_leaf: [1, 10]
    criterion: ['gini', 'entropy']
  decision_tree_settings:
    # Keyword arguments for scikit-learn DecisionTreeClassifier
    max_depth: 3

  # ADA BOOST
  adaboost_settings:
    n_estimators: 100

  # XG BOOST
  xgboost_settings:
    n_estimators: 200
    learning_rate: 0.1
    max_depth: 6
    objective: "binary:logistic"
    tree_method: "hist"
    subsample: 0.8
    colsample_bytree: 0.8

  mlp_setting:
    hidden_layers: [64, 64, 64]
    lr: 0.01
    epochs: 100 
