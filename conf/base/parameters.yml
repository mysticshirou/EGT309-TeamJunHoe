cleaning_params:
  # AGE
  # Create the age_unk columns (True, False)
  create_age_unk: False
  # Do KNN imputation to impute missing ages (True, False)
  age_knn_impute: True
  age_knn_impute_settings:  
    # Keyword arguments for KNNImputer from sklearn.impute
    n_neighbors: 2
    weights: "distance"

  # OCCUPATION
  generalize: False

  # PREVIOUS CONTACT DAYS
  # Create previously_contacted column (True, False)
  create_previously_contacted: True

  # PERSONAL_LOAN 
  # Cleaning mode for personal_loan column ('fill', 'drop', 'impute')
  personal_loan_cleaning: "drop"

encoding_params:
  # Options: ('ohe', 'label')
  encode: "ohe"

splitting_params:
  # Float value representing % of dataset to be in test split
  test_size: 0.2
  # Options: ('undersampling', 'stratified', 'class_weights')
  imbalance_handling: 'undersampling'

model_params:
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 764d4af (Merging latest changes)
  # Selected model for training 
  # "decision_tree" | "ada_boost" | "xg_boost" | "mlp"
  model_choice: "decision_tree"
  # Global random state seed
  random_state: 0
<<<<<<< HEAD
=======
  # Selected model for training "decision_tree" | "ada_boost" | "xg_boost" | "mlp"
  model_choice: "ada_boost"
  decision_tree_settings:
    max_depth: 5
    min_samples_leaf: 10
    class_weight: "balanced"
    random_state: 0
>>>>>>> 2b386ea (Improved reporting by adding both f1 score and accuracy to results output)
=======
>>>>>>> 764d4af (Merging latest changes)

  # DECISION TREE
  # Use bayesian optimisation to auto select best parameters: True | False
  decision_tree_auto_optimize: True
  decision_tree_bayes_search_settings:
    # Keyword arguments for bayesian optimisation for scikit-optimize
    scoring: "f1"
    n_iter: 50
    cv: 5
  decision_tree_bayes_search_search_space:
    # Search space for decision tree bayesian optimisation
    max_depth: [1, 20]
    min_samples_split: [2, 20]
    min_samples_leaf: [1, 10]
    criterion: ['gini', 'entropy']
  decision_tree_settings:
    # Keyword arguments for scikit-learn DecisionTreeClassifier
    max_depth: 3

  # ADA BOOST
  adaboost_settings:
<<<<<<< HEAD
<<<<<<< HEAD
    n_estimators: 100
=======
    n_estimators: 200
    learning_rate: 0.1
    algorithm: "SAMME.R"
    base_estimator:
      max_depth: 3
      min_samples_leaf: 10
      class_weight: "balanced"
      random_state: 0
    random_state: 0
>>>>>>> 2b386ea (Improved reporting by adding both f1 score and accuracy to results output)

  # XG BOOST
  xgboost_settings:
    n_estimators: 300
    learning_rate: 0.05
    max_depth: 5
=======
    n_estimators: 100

  # XG BOOST
  xgboost_settings:
    n_estimators: 200
    learning_rate: 0.1
    max_depth: 6
>>>>>>> 764d4af (Merging latest changes)
    objective: "binary:logistic"
    tree_method: "hist"
    subsample: 0.8
    colsample_bytree: 0.8

  mlp_setting:
    hidden_layers: [64, 64, 64]
    lr: 0.01
    epochs: 100 
